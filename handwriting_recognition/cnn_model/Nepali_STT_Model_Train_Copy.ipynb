{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2a2d96",
   "metadata": {},
   "source": [
    "# Nepali Handwriting Recognition Model Training Notebook\n",
    "\n",
    "This notebook prepares a copy of the Nepali-STT-Model for training within the notebook environment. Follow the steps below to set up, inspect, and prepare the model for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c7878",
   "metadata": {},
   "source": [
    "## 1. Clone the Repository\n",
    "Clone the Nepali-STT-Model repository (if not already present in your environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab or a fresh environment, uncomment below:\n",
    "# !git clone https://github.com/sandeshbhatta495/Nepali-STT-Model.git\n",
    "# %cd Nepali-STT-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fa591",
   "metadata": {},
   "source": [
    "## 2. Install Required Dependencies\n",
    "Install all necessary Python packages for model training and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab or a fresh environment, uncomment below:\n",
    "# !pip install torch torchvision pillow numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd46ad9",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect the Model Code\n",
    "Read and review the model definition (model.py) to understand the architecture and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ac926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Lightweight CNN classifier for Nepali word recognition.\n",
      "\n",
      "Architecture:\n",
      "  Input:  1 × 64 × 192   (grayscale, height × width)\n",
      "  Conv → BN → ReLU → Pool  ×3\n",
      "  Global Average Pool\n",
      "  FC → Dropout → FC (num_classes)\n",
      "  CrossEntropy loss\n",
      "\n",
      "Total params: ~200K–500K  →  < 2 MB saved model.\n",
      "\"\"\"\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class NepaliWordCNN(nn.Module):\n",
      "    \"\"\"\n",
      "    3-layer CNN for word-level classification.\n",
      "\n",
      "    Image flows:\n",
      "        1×64×192\n",
      "      → Conv 32 (3×3) → BN → ReLU → MaxPool(2) → 32×32×96\n",
      "      → Conv 64 (3×3) → BN → ReLU → MaxPool(2) → 64×16×48\n",
      "      → Conv 128(3×3) → BN → ReLU → MaxPool(2) → 128×8×24\n",
      "      → AdaptiveAvgPool(1) → 128\n",
      "      → FC 128 → ReLU → Dropout → FC num_classes\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, num_classes: int, dropout: float = 0.3):\n",
      "        super().__init__()\n",
      "\n",
      "        self.features = nn.Sequential(\n",
      "            # Block 1\n",
      "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
      "            nn.BatchNorm2d(32),\n",
      "            nn.ReLU(inplace=True),\n",
      "            nn.MaxPool2d(2),\n",
      "\n",
      "            # Block 2\n",
      "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
      "            nn.BatchNorm2d(64),\n",
      "            nn.ReLU(inplace=True),\n",
      "            nn.MaxPool2d(2),\n",
      "\n",
      "            # Block 3\n",
      "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
      "            nn.BatchNorm2d(128),\n",
      "            nn.ReLU(inplace=True),\n",
      "            nn.MaxPool2d(2),\n",
      "        )\n",
      "\n",
      "        self.global_pool = nn.AdaptiveAvgPool2d(1)  # → 128×1×1\n",
      "\n",
      "        self.classifier = nn.Sequential(\n",
      "            nn.Linear(128, 128),\n",
      "            nn.ReLU(inplace=True),\n",
      "            nn.Dropout(dropout),\n",
      "            nn.Linear(128, num_classes),\n",
      "        )\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            x: (B, 1, 64, 192)  float [0, 1]\n",
      "        Returns:\n",
      "            logits: (B, num_classes)\n",
      "        \"\"\"\n",
      "        x = self.features(x)\n",
      "        x = self.global_pool(x)          # (B, 128, 1, 1)\n",
      "        x = x.view(x.size(0), -1)        # (B, 128)\n",
      "        x = self.classifier(x)           # (B, num_classes)\n",
      "        return x\n",
      "\n",
      "    def predict_topk(self, x: torch.Tensor, k: int = 5):\n",
      "        \"\"\"\n",
      "        Return top-k predictions with probabilities.\n",
      "\n",
      "        Returns:\n",
      "            probs: (B, k) float\n",
      "            indices: (B, k) int\n",
      "        \"\"\"\n",
      "        logits = self.forward(x)\n",
      "        probs = F.softmax(logits, dim=-1)\n",
      "        topk_probs, topk_idx = torch.topk(probs, k, dim=-1)\n",
      "        return topk_probs, topk_idx\n",
      "\n",
      "\n",
      "def count_parameters(model: nn.Module) -> int:\n",
      "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    from vocab import VOCAB_SIZE\n",
      "    model = NepaliWordCNN(num_classes=VOCAB_SIZE)\n",
      "    print(model)\n",
      "    print(f\"\\nTrainable parameters: {count_parameters(model):,}\")\n",
      "\n",
      "    # Test forward pass\n",
      "    dummy = torch.randn(2, 1, 64, 192)\n",
      "    out = model(dummy)\n",
      "    print(f\"Input: {dummy.shape}  →  Output: {out.shape}\")\n",
      "\n",
      "    probs, idx = model.predict_topk(dummy, k=3)\n",
      "    print(f\"Top-3 probs: {probs}\")\n",
      "    print(f\"Top-3 idx:   {idx}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the model code (for reference)\n",
    "with open('handwriting_recognition/cnn_model/model.py', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9c599",
   "metadata": {},
   "source": [
    "## 4. Copy Model Definition to Notebook\n",
    "Copy the NepaliWordCNN model class and any required helper functions directly into this notebook for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NepaliWordCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    3-layer CNN for word-level classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    def predict_topk(self, x: torch.Tensor, k: int = 5):\n",
    "        logits = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_idx = torch.topk(probs, k, dim=-1)\n",
    "        return topk_probs, topk_idx\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40dab8b",
   "metadata": {},
   "source": [
    "## 5. Verify Model Instantiation\n",
    "Instantiate the model to confirm it runs without errors and is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NepaliWordCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=237, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 140,205\n"
     ]
    }
   ],
   "source": [
    "# Example: Instantiate the model with a dummy class count (replace with your vocab size)\n",
    "VOCAB_SIZE = 237  # Replace with actual vocab size if different\n",
    "model = NepaliWordCNN(num_classes=VOCAB_SIZE)\n",
    "print(model)\n",
    "print(f\"Trainable parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3bad4-8272-415a-ba5a-ba448200e8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150825dd-6fd3-4d6f-bc6d-13c4644f51b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03985b43-c5ac-447f-b32c-9e79f142f9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd0c9e-2e22-46ea-996e-fb2b571cd878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
